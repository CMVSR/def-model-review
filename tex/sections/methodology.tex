\section{Methodologies Explored}
main key points -- definition generation and word embedding (similarity disabmiguity etc. polysemy). Few line of text should be here.  

\begin{longtable}{|p{3.5cm}|p{3.5cm}|p{3.5cm}|p{3.5cm}|}
    \caption{Definition Modeling Methods}                                                                                                                                                                                                                                                                                                                                                   \\
    \hline
    Article                                                                                                                   & Evaluation                                                       & Dataset                                                                                                                                       & Models                                   \\
    \hline
    \citeauthor*{noraset_definition_2016} \citeyear{noraset_definition_2016} \cite{noraset_definition_2016}                   & Perplexity, BLEU                                                 & GCIDE, Custom                                                                                                                                 & RNN, Word2Vec, LSTM                      \\
    \hline
    \citeauthor*{gadetsky_conditional_2018} \citeyear{gadetsky_conditional_2018} \cite{gadetsky_conditional_2018}             & Perplexity, BLEU                                                 & Oxford                                                                                                                                        & LSTM, Word2Vec, SkipGram                 \\
    \hline
    \citeauthor*{chang_what_2019} \citeyear{chang_what_2019} \cite{chang_what_2019}                                           & Precision, ROUGE-L, Cosine similarity                            & Oxford                                                                                                                                        & ELMo, BERT, FastText                     \\
    \hline
    \citeauthor*{washio_bridging_2019} \citeyear{washio_bridging_2019} \cite{washio_bridging_2019}                            & Perplexity, BLEU                                                 & \cite{noraset_definition_2016}, \cite{gadetsky_conditional_2018}                                                                              & Encoder/decoder                          \\
    \hline
    \citeauthor*{mickus_mark_2019} \citeyear{mickus_mark_2019} \cite{mickus_mark_2019}                                        & Perplexity                                                       & \cite{noraset_definition_2016}, \cite{gadetsky_conditional_2018}, Custom                                                                      & GloVe, Transformer, sequence-to-sequence \\
    \hline
    \citeauthor*{li_explicit_2020} \citeyear{li_explicit_2020} \cite{li_explicit_2020}                                        & BLEU, METEOR, Human                                              & WordNet, Oxford                                                                                                                               & CNN, LSTM                                \\
    \hline
    \citeauthor*{bevilacqua_generationary_2020} \citeyear{bevilacqua_generationary_2020} \cite{bevilacqua_generationary_2020} & Perplexity, BLEU, ROUGE-L, METEOR, BERTScore                     & Oxford \cite{chang_what_2019}, Sem-Cor \cite{miller_semantic_1993}, Wiktionary, GCIDE \cite{noraset_definition_2016}, Hei++ (Custom), WordNet & BART                                     \\
    \hline
    \citeauthor*{sojka_evaluating_2020} \citeyear{sojka_evaluating_2020} \cite{sojka_evaluating_2020}                         & BLEU, rBLEU (recall-based), fBLEU (harmonic mean of BLEU, rBLEU) & Wiktionary, OmegaWiki, WordNet                                                                                                                & AdaGram, Word2Vec, CNN, RNN              \\
    \hline
    \citeauthor*{huang_cdm_2021} \citeyear{huang_cdm_2021} \cite{huang_cdm_2021}                                              & BLEU, ROUGE-L, METEOR, BERTScore                                 & Wikipedia                                                                                                                                     & BERT                                     \\
    \hline
    \citeauthor*{reid_vcdm_2020} \citeyear{reid_vcdm_2020} \cite{reid_vcdm_2020}                                              & BLEU, BERTScore (Precision, Recall, F1), Human                   & Oxford, Urban, Wikipedia, Cambridge, Robert (French)                                                                                          & BERT, LSTM                               \\
    \hline
    % \citeauthor*{noraset_definition_2016} \citeyear{noraset_definition_2016} \cite{noraset_definition_2016}                   & Perplexity, BLEU                             & GCIDE, Custom                                                                                                                                 & RNN, Word2Vec, LSTM                      \\
\end{longtable}

\subsection{Language Models}
A definition model is a language model that is trained on a set of definitions
\cite{noraset_definition_2016}. The goal of a definition model is to learn to
generate a definition ($\textbf{d} = [d_1, ..., d_T]$) for a given term $w$. The
probability of generating the $t$-th word in a definition depends on both the
previous words in the definition and the word to be defined (Eq.
\ref{eq:definition_model}).

\begin{equation}
    \label{eq:definition_model}
    p(\textbf{d} | w) = \prod_{t=1}^{T} p(d_t | d_1,...,d_{t-1}, w)
\end{equation}

Noraset et al. condition an RNN to generate a defintion from an input seed word.
They modify the model by updating the output of the recurrent unit with an
update function inspired by GRU update gate \cite{noraset_definition_2016}. They
apply pretrained word embeddings generated from Word2Vec.

In later work, it was shown that the definition model does not generate
definitions for words with ambigious word sense, especially polysemantic words.
The following context-aware definition model was proposed by
\citeauthor*{gadetsky_conditional_2018} in order to tackle this challenge
\cite{gadetsky_conditional_2018}. They extend Equation \ref{eq:definition_model}
by adding a context term ($\textbf{c} = [c_1, ..., c_T]$) which is a context or
example sentence to be used in the generation of the definition.

\begin{equation}
    \label{eq:context_aware_definition_model}
    p(\textbf{d} | w, \textbf{c}) = \prod_{t=1}^{T} p(d_t | d_1,...,d_{t-1}, w, \textbf{c})
\end{equation}

In order to generate a definition, they use an attention-based SkipGram model to
extract dimensions from the embedding which contain the most relevant
information.

\cite{huang_cdm_2021} focused on language generation, etc.\cite{gadetsky_conditional_2018} conditional generation of definition.\cite{zheng_decompose_2021}, chienese defition generation.
\cite{sojka_evaluating_2020}, generation
\cite{li_explicit_2020}
\cite{bevilacqua_generationary_2020}
\cite{zhang_improving_2020}
\cite{yang_incorporating_2020}
\cite{ishiwatari_learning_2019}a general task of defining un-
known phrases given their contexts.

\subsection{Word Embedding}
In recent years, definition modeling has gained popularity, and researchers proposed various methods to map between definition and the target word. One of the significant issues in mapping words with the dictionary is contextual ambiguity and embedding-based word representation. In NLP, word embedding represents the vector representation of words that encodes the word's meaning such that similar words have similar vector space. There are various word embedding techniques used to resolve ambiguity between words. In the definition modeling problem, word to vector representation is a key factor in modeling definition words/terms for a given term. 

\cite{bosc_auto_2018} exploited dictionary recursivity into consideration and proposed an autoencoder-based word embedding algorithm, and generated a single embedding per word—the proposed auto-encoder model comprises of LSTM encoder and decoder. The authors introduced three embeddings - i) definition embeddings produced by the proposed definition encoder, ii) input embeddings for the encoder, and iii) output embeddings. While modeling these embedding models, the author incorporates consistency penalty as soft weight in their cost function to enforce input embedding and definition embedding closer. 

In \cite{washio_bridging_2019}, the authors consider lexical-semantic relations between the defined word and defining words using unsupervised methods to propose definition modeling. To learn embedding author proposed LSTM based encoder and decoder with additional cost function to learn word-pair embeddings in the decoder and capture lexical-semantic relations.

Dictionary embeddings often follow a genus + differentia structure for a dictionary definition~\cite{noraset_definition_2016}—author capture hypernyms embedding following proper genus database WebIsA containing hypernym relations. In addition, the author incorporates char-CNN to capture affixes to model gated-RNN based definition modeling.


Word-embeddings are learned from large corpora. Therefore, it may consists of biases such as gender, race, religion, etc. On the otherhand, word dictionaries contain unbiased, concise definitions. To overcome these biases by utilizing pre-trained word-embedding, authors learned embedding from existing input word embeddings using encoder-decoder architecture by defining decoder cost function that considers dictionary agreement as a constraint and decoded the debiased embedding~\cite{kaneko_dictionary_2021}. 

% \subsection{Methodology Comparisons}
% Mostly focus on different group of papers focused on similar task with similar
% datasets. show tables that mentioned - dataset names, algorithm used
% (highlevel), performances (acc, f1 or so on). Need to think after writing those
% previous sections.

% \subsection{Deep Learning}
% general diagram for deep learning approach.


% Semi-supervised approach \cite{patra_bilingual_2019}.

% \cite{wu_2021_sequence}We computationally model the processes of
% word borrowing from a donor word to an incorporated
% word, and vice versa, by answering
% two questions: (1) what does a word look
% like incorporated into another language, and
% in the opposite direction (2) where did a word
% come from? We experiment with several
% model variants, including LSTM encoderdecoders,
% copy attention, and Transformers.

% \cite{wu_computational_2020} For our model, we used a LSTM with an embedding
% dimension of 128 and hidden dimension of 128.
% The output of the last hidden state is passed to a fully connected layer with a sigmoid activation function.

% \subsubsection{Parameters and Evaluation}
% \begin{itemize}
%     \item Perplexity
%     \item Precision
%     \item BLEU score
%     \item ROUGE-L
%     \item Cosine similarity
% \end{itemize}

% In general, the performance of generative definition models are evaluated using
% perplexity and BLEU score.
