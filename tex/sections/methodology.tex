\section{Methodologies Explored}
Methods -- ML, Deep Learning, Active Learning, Reinforcement Learning, etc.

\begin{longtable}{|p{3.5cm}|p{3.5cm}|p{3.5cm}|p{3.5cm}|}
    \caption{Definition Modeling Methods}                                                                                                                                                                                                                                                                                                                                                  \\
    \hline
    Article                                                                                                                  & Evaluation                                                       & Dataset                                                                                                                                       & Models                                   \\
    \hline
    \citeauthor{noraset_definition_2016} \citeyear{noraset_definition_2016} \cite{noraset_definition_2016}                   & Perplexity, BLEU                                                 & GCIDE, Custom                                                                                                                                 & RNN, Word2Vec, LSTM                      \\
    \hline
    \citeauthor{gadetsky_conditional_2018} \citeyear{gadetsky_conditional_2018} \cite{gadetsky_conditional_2018}             & Perplexity, BLEU                                                 & Oxford                                                                                                                                        & LSTM, Word2Vec, SkipGram                 \\
    \hline
    \citeauthor{chang_what_2019} \citeyear{chang_what_2019} \cite{chang_what_2019}                                           & Precision, ROUGE-L, Cosine similarity                            & Oxford                                                                                                                                        & ELMo, BERT, FastText                     \\
    \hline
    \citeauthor{washio_bridging_2019} \citeyear{washio_bridging_2019} \cite{washio_bridging_2019}                            & Perplexity, BLEU                                                 & \cite{noraset_definition_2016}, \cite{gadetsky_conditional_2018}                                                                              & Encoder/decoder                          \\
    \hline
    \citeauthor{mickus_mark_2019} \citeyear{mickus_mark_2019} \cite{mickus_mark_2019}                                        & Perplexity                                                       & \cite{noraset_definition_2016}, \cite{gadetsky_conditional_2018}, Custom                                                                      & GloVe, Transformer, sequence-to-sequence \\
    \hline
    \citeauthor{li_explicit_2020} \citeyear{li_explicit_2020} \cite{li_explicit_2020}                                        & BLEU, METEOR, Human                                              & WordNet, Oxford                                                                                                                               & CNN, LSTM                                \\
    \hline
    \citeauthor{bevilacqua_generationary_2020} \citeyear{bevilacqua_generationary_2020} \cite{bevilacqua_generationary_2020} & Perplexity, BLEU, ROUGE-L, METEOR, BERTScore                     & Oxford \cite{chang_what_2019}, Sem-Cor \cite{miller_semantic_1993}, Wiktionary, GCIDE \cite{noraset_definition_2016}, Hei++ (Custom), WordNet & BART                                     \\
    \hline
    \citeauthor{sojka_evaluating_2020} \citeyear{sojka_evaluating_2020} \cite{sojka_evaluating_2020}                         & BLEU, rBLEU (recall-based), fBLEU (harmonic mean of BLEU, rBLEU) & Wiktionary, OmegaWiki, WordNet                                                                                                                & AdaGram, Word2Vec, CNN, RNN              \\
    \hline
    \citeauthor{huang_cdm_2021} \citeyear{huang_cdm_2021} \cite{huang_cdm_2021}                                              & BLEU, ROUGE-L, METEOR, BERTScore                                 & Wikipedia                                                                                                                                     & BERT                                     \\
    \hline
    \citeauthor{reid_vcdm_2020} \citeyear{reid_vcdm_2020} \cite{reid_vcdm_2020}                                              & BLEU, BERTScore (Precision, Recall, F1), Human                   & Oxford, Urban, Wikipedia, Cambridge, Robert (French)                                                                                          & BERT, LSTM                               \\
    \hline
    % \citeauthor{noraset_definition_2016} \citeyear{noraset_definition_2016} \cite{noraset_definition_2016}                   & Perplexity, BLEU                             & GCIDE, Custom                                                                                                                                 & RNN, Word2Vec, LSTM                      \\
\end{longtable}

\subsection{Language Models}
A definition model is a language model that is trained on a set of definitions.
The goal of a definition model is to learn to generate a definition %
($\textbf{d} = [d_1, ..., d_T])$ for a given term $w$. The probability of
generating the $t$-th word in a definition depends on both the previous words in
the definition and the word to be defined (Eq. \ref{eq:definition_model}).

\begin{equation}
    \label{eq:definition_model}
    p(\textbf{d} | w) = \prod_{t=1}^{T} p(d_t | d_1,...,d_{t-1}, w)
\end{equation}

\subsection{Machine Learning}
Noraset et al. condition an RNN to generate a defintion from an input seed word.
They modify the model by updating the output of the recurrent unit with an
update function inspired by GRU update gate \cite{noraset_definition_2016}. They
apply pretrained word embeddings generated from Word2Vec.

Semi-supervised approach \cite{patra_bilingual_2019}.

\subsection{Deep Learning}
general diagram for deep learning approach.

\cite{wu_2021_sequence}We computationally model the processes of
word borrowing from a donor word to an incorporated
word, and vice versa, by answering
two questions: (1) what does a word look
like incorporated into another language, and
in the opposite direction (2) where did a word
come from? We experiment with several
model variants, including LSTM encoderdecoders,
copy attention, and Transformers.

\cite{wu_computational_2020} For our model, we used a LSTM with an embedding
dimension of 128 and hidden dimension of 128.
The output of the last hidden state is passed to a fully connected layer with a sigmoid activation function.

\subsection{Methodology Comparisons}
Mostly focus on different group of papers focused on similar task with similar
datasets. show tables that mentioned - dataset names, algorithm used
(highlevel), performances (acc, f1 or so on). Need to think after writing those
previous sections.

\subsubsection{Parameters and Evaluation}
\begin{itemize}
    \item Perplexity
    \item Precision
    \item BLEU score
    \item ROUGE-L
    \item Cosine similarity
\end{itemize}

In general, the performance of generative definition models are evaluated using
perplexity and BLEU score.
