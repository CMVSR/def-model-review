\section{Methodologies Explored}
We explored recent literature related to definition modeling and presented our findings related to explored methodology in this section. Definition generation is a critical task where multiple definitions can be generated for a single target word. Therefore, researchers focus on improving the definition generation task by applying various techniques. Two key technical aspects are observed in the literature -\emph{ i)} definition generation, and \emph{ii)} word embedding. Definition generation is considered a language modeling task, where we predict the joint probability of a  sequence of words, and based on maximum likelihood, the highest probability sequence returned as a definition of a given target word. Since definition mostly depends on the context of the target word, vector representation of such target words is essential to capture context scenarios. Below we discuss both of these aspects, language modeling and word embedding techniques and related literature.

\begin{longtable}{|p{3.5cm}|p{3.5cm}|p{3.5cm}|p{3.5cm}|}
    \caption{Definition Modeling Methods}                                                                                                                                                                                                                                                                                                                                                   \\
    \hline
    Article                                                                                                                   & Evaluation                                                       & Dataset                                                                                                                                       & Models                                   \\
    \hline
    \citeauthor*{noraset_definition_2016} \citeyear{noraset_definition_2016} \cite{noraset_definition_2016}                   & Perplexity, BLEU                                                 & GCIDE, Custom                                                                                                                                 & RNN, Word2Vec, LSTM                      \\
    \hline
    \citeauthor*{gadetsky_conditional_2018} \citeyear{gadetsky_conditional_2018} \cite{gadetsky_conditional_2018}             & Perplexity, BLEU                                                 & Oxford                                                                                                                                        & LSTM, Word2Vec, SkipGram                 \\
    \hline
    \citeauthor*{chang_what_2019} \citeyear{chang_what_2019} \cite{chang_what_2019}                                           & Precision, ROUGE-L, Cosine similarity                            & Oxford                                                                                                                                        & ELMo, BERT, FastText                     \\
    \hline
    \citeauthor*{washio_bridging_2019} \citeyear{washio_bridging_2019} \cite{washio_bridging_2019}                            & Perplexity, BLEU                                                 & \cite{noraset_definition_2016}, \cite{gadetsky_conditional_2018}                                                                              & Encoder/decoder                          \\
    \hline
    \citeauthor*{mickus_mark_2019} \citeyear{mickus_mark_2019} \cite{mickus_mark_2019}                                        & Perplexity                                                       & \cite{noraset_definition_2016}, \cite{gadetsky_conditional_2018}, Custom                                                                      & GloVe, Transformer, sequence-to-sequence \\
    \hline
    \citeauthor*{li_explicit_2020} \citeyear{li_explicit_2020} \cite{li_explicit_2020}                                        & BLEU, METEOR, Human                                              & WordNet, Oxford                                                                                                                               & CNN, LSTM                                \\
    \hline
    \citeauthor*{bevilacqua_generationary_2020} \citeyear{bevilacqua_generationary_2020} \cite{bevilacqua_generationary_2020} & Perplexity, BLEU, ROUGE-L, METEOR, BERTScore                     & Oxford \cite{chang_what_2019}, Sem-Cor \cite{miller_semantic_1993}, Wiktionary, GCIDE \cite{noraset_definition_2016}, Hei++ (Custom), WordNet & BART                                     \\
    \hline
    \citeauthor*{sojka_evaluating_2020} \citeyear{sojka_evaluating_2020} \cite{sojka_evaluating_2020}                         & BLEU, rBLEU (recall-based), fBLEU (harmonic mean of BLEU, rBLEU) & Wiktionary, OmegaWiki, WordNet                                                                                                                & AdaGram, Word2Vec, CNN, RNN              \\
    \hline
    \citeauthor*{huang_cdm_2021} \citeyear{huang_cdm_2021} \cite{huang_cdm_2021}                                              & BLEU, ROUGE-L, METEOR, BERTScore                                 & Wikipedia                                                                                                                                     & BERT                                     \\
    \hline
    \citeauthor*{reid_vcdm_2020} \citeyear{reid_vcdm_2020} \cite{reid_vcdm_2020}                                              & BLEU, BERTScore (Precision, Recall, F1), Human                   & Oxford, Urban, Wikipedia, Cambridge, Robert (French)                                                                                          & BERT, LSTM                               \\
    \hline
    % \citeauthor*{noraset_definition_2016} \citeyear{noraset_definition_2016} \cite{noraset_definition_2016}                   & Perplexity, BLEU                             & GCIDE, Custom                                                                                                                                 & RNN, Word2Vec, LSTM                      \\
\end{longtable}

\subsection{Language Models and definition generation}
A definition model is a language model that is trained on a set of definitions
\cite{noraset_definition_2016}. The goal of a definition model is to learn to
generate a definition ($\textbf{d} = [d_1, ..., d_T]$) for a given term $w$. The
probability of generating the $t$-th word in a definition depends on both the
previous words in the definition and the word to be defined (Eq.
\ref{eq:definition_model}).

\begin{equation}
    \label{eq:definition_model}
    p(\textbf{d} | w) = \prod_{t=1}^{T} p(d_t | d_1,...,d_{t-1}, w)
\end{equation}

Researchers apply sequence-to-sequence algorithms and represented definitions vectors by formulating language modeling to capture sequence features and context. Among these algorithms, recurrent neural network (RNN), Long-short-term-memory network (LSTM), etc., are important. Not all words are equally important in a definition as they have different contributions in the definition generations. Transformer-based techniques help focus on the contribution of particular words in the definition. Therefore, few researchers also focus on transformer networks such as Bidirectional Encoder Representations from Transformers (BERT)~\citeauthor*{devlin2018bert}, denoising decoder (BART)~\citeauthor*{lewis2019bart}, etc.

Noraset et al. condition an RNN to generate a defintion from an input seed word.
They modify the model by updating the output of the recurrent unit with an
update function inspired by GRU update gate \cite{noraset_definition_2016}. They
apply pretrained word embeddings generated from Word2Vec. In later work, it was shown that the definition model does not generate
definitions for words with ambigious word sense, especially polysemantic words.
The following context-aware definition model was proposed by
\citeauthor*{gadetsky_conditional_2018} in order to tackle this challenge
\cite{gadetsky_conditional_2018}. They extend Equation \ref{eq:definition_model}
by adding a context term ($\textbf{c} = [c_1, ..., c_T]$) which is a context or
example sentence to be used in the generation of the definition.

\begin{equation}
    \label{eq:context_aware_definition_model}
    p(\textbf{d} | w, \textbf{c}) = \prod_{t=1}^{T} p(d_t | d_1,...,d_{t-1}, w, \textbf{c})
\end{equation}

In order to generate a definition, authors use an attention-based SkipGram model to
extract dimensions from the embedding which contain the most relevant
information.

% \subsection{Definition Generation:}



The definition usually contains summarized information about the given target word. \citeauthor*{huang_cdm_2021} focus on generating definition by using extracted self- and correlative definition information of a given term/word from the Web \cite{huang_cdm_2021}. The author extracted sentences containing the target term and then ranked sentences using deployed BERT-based model and extracted self-definitional information (SDI) from Wikipedia. Then, they design conditional sequence to sequence model, BART, and fine-tune parameter with extracted information and general definition for a given term. Definition modeling works similarly to Language models to generate definition sentences and corresponding probabilities. \citeauthor*{gadetsky_conditional_2018} proposed a conditional recurrent neural network (RNN) based language model for developing the definition of a given the word \cite{gadetsky_conditional_2018}. First, they created AdaGram based RNN model and conditioned it on Adaptive Skip-gram vector representation. Their second model focused on attention-based Skip-gram to generate a definition for a corresponding context. \citeauthor*{sojka_evaluating_2020} proposed context agnostic multi-sense definition generation model \cite{sojka_evaluating_2020}. The proposed RNN based model generates multiple definitions based on a given target word type (polysemy word) and incorporates the char-CNN model to capture affixes knowledge. They associate sense vectors with definitions and create a definition-to-sense and sense-to-definition model. These definition models represented definition by taking the average of the word embeddings of all the words.

\citeauthor*{li_explicit_2020} proposed explicit semantic decomposition (ESD) to decompose the meaning of the word into semantic components and model them with the discrete latent variable for definition generation \cite{li_explicit_2020}. This model comprises an encoder, decoder, and semantic component predictor. The encoder consists of two components - word encoder and Bidirectional-LSTM context encoder. Word encoder creates low-dimensional vectors of the word, whereas the BiLSTM context encoder incorporates context information. Semantic component predictor model approximate posterior using Bi-LSTM model. Finally, LSTM based definition decoder generates definition from the encoded data. \citeauthor*{bevilacqua_generationary_2020} propose a span-based encoding
model (BART) that is used to map occurrences of target words or phrases and generate gloss \cite{bevilacqua_generationary_2020}. Finally, Gloss Probability Scoring is applied to select the highest probable gloss and thus create the word's definition. \citeauthor*{zhang_improving_2020} formulated multitask GRU-based sequence to sequence modeling to generate definition and example sentences \cite{zhang_improving_2020}. In this process, authors deploy Elmo model to get context sensitive embedding vector of a target word. \citeauthor*{ishiwatari_learning_2019} solve the problem of unknown phrase definition by incorporating local and global context information while defining a word \cite{ishiwatari_learning_2019}. Local context refers to the sequence of neighboring words of the target word. In contrast, the global context refers to the entire document or even search the web text to find other occurrences of the expression to understand the meaning. Authors proposed similar to LSTM based encoder-decoder model where gated unit deployed reduces the ambiguity of local and global context.



\citeauthor*{mickus_mark_2019} reformalize the problem of definition modeling to a sequence-to-sequence task by defining a highlighted word in an input context sentence \cite{mickus_mark_2019}. \citeauthor*{mickus_mark_2019} argue that due to the distribution hypothesis (words with similar distribution have similar meaning), the problem of definition modeling should be reformulated as a sequence-to-sequence task, where the input sequence is a sentence with the word to be defined highlighted \cite{mickus_mark_2019}. The input sequence provides the context necessary to generate the output definition. \citeauthor*{zhu_multi_2019} study the multi-sense definition modeling task using the Gumble-softmax approach \cite{zhu_multi_2019}. This approach decomposes word senses from the pre-trained word embeddings and applies LSTM-sequence to sequence modeling to generate definition sentences. \citeauthor*{reid_vcdm_2020} introduced the variational generative model to produce a definition that directly combines lexical and distributional semantics using the continuous latent variable \cite{reid_vcdm_2020}. Initially, the BERT model is fine-tuned with phrase-context pairs, and in the context, sentence lexeme form is used to reduce the differences in the word or phrase. Once the BERT model encodes the definition, the proposed approach applies a neural definition inference module to compute approximate posterior from the variational distribution of the definition. During definition generation, that is, sequence of word generation task, this model deploys LSTM enabled variational contextual definition modeler to generate a sequence of words as the definition. \citeauthor*{chang_what_2019} explore contextualized embedding for definition modeling. To get contextualized word embedding author used the pretrained ELMo and BERT model \cite{chang_what_2019}. the authors reformulate the problem of definition modeling from text generation to text classification. Instead of mapping the classifier with discrete labels, authors encode all ground truth definitions in the embedding space via learning a mapping function. Then this approach applies k-nearest neighbor to predict the appropriate definition for a given target word. Their results show state-of-the-art performance on the task of definition modeling.


% non-english
\textbf{Non-English Languages:} Definition generation was also explored in the non-English language. Since definition depends on the lexical property, language syntax, construction of the phrases, and so on, different languages influence the proposed methodology to capture the definition of a specific word. In parataxis languages (i.e., Chinese), words meaning composed of formation process - formation component (morphemes) combined by formation rule (morphemes are combined to form words). \citeauthor*{zheng_decompose_2021} utilizes this word meaning formation process in consideration to build a definition generation model where words decompose into formation features and then use gating techniques to generate definition \cite{zheng_decompose_2021}. In this work, the authors develop morpheme features using the bi-LSTM model and concatenate character-level embedding and pre-trained word embedding together. Finally, gated attention-based morpheme features with attention-based context vector to form a feature vector. The definition generator employs a gated LSTM model that uses the feature vector and generates definition. \citeauthor*{ni_learning_2017} automatically generates explanations for non-standard English expression using sequence to sequence models \cite{ni_learning_2017}. The authors use two encoder approaches - word-level LSTM-encoder encodes context information while character-level encoder encodes target non-standard terms. \citeauthor*{kong_toward_2020} fine-tune mBERT and XLM cross-lingual model and provide target word and examples sentence as context to produce definition as output \cite{kong_toward_2020}. This model can generate definitions in English from various languages (e.g., Chinese to English). 


\subsection{Word Embedding}
In recent years, definition modeling has gained popularity, and researchers proposed various methods to map between definition and the target word. One of the significant issues in mapping words with the dictionary is contextual ambiguity and embedding-based word representation. In NLP, word embedding represents the vector representation of words that encodes the word's meaning such that similar words have similar vector space. There are various word embedding techniques used to resolve ambiguity between words. In the definition modeling problem, word to vector representation is a key factor in modeling definition words/terms for a given term. 

\citeauthor*{bosc_auto_2018} exploited dictionary recursivity into consideration and proposed an autoencoder-based word embedding algorithm, and generated a single embedding per word—the proposed auto-encoder model comprises of LSTM encoder and decoder \cite{bosc_auto_2018}. The authors introduced three embeddings - i) definition embeddings produced by the proposed definition encoder, ii) input embeddings for the encoder, and iii) output embeddings. While modeling these embedding models, the author incorporates consistency penalty as soft weight in their cost function to enforce input embedding and definition embedding closer. \citeauthor*{washio_bridging_2019}, the authors consider lexical-semantic relations between the defined word and defining words using unsupervised methods to propose definition modeling \cite{washio_bridging_2019}. To learn embedding author proposed LSTM based encoder and decoder with additional cost function to learn word-pair embeddings in the decoder and capture lexical-semantic relations. Dictionary embeddings often follow a genus + differentia structure for a dictionary definition \citeauthor*{noraset_definition_2016} capture hypernyms embedding following proper genus database WebIsA containing hypernym relations \cite{noraset_definition_2016}. In addition, the author incorporates char-CNN to capture affixes to model gated-RNN based definition modeling.


Word-embeddings are learned from large corpora. Therefore, it may consists of biases such as gender, race, religion, etc. On the otherhand, word dictionaries contain unbiased, concise definitions. To overcome these biases by utilizing pre-trained word-embedding, authors learned embedding from existing input word embeddings using encoder-decoder architecture by defining decoder cost function that considers dictionary agreement as a constraint and decoded the debiased embedding \cite{kaneko_dictionary_2021}. \citeauthor*{zhang_improving_2020} propose a novel framework by formulating definition modeling and word-embedding as multitask learning problems \cite{zhang_improving_2020}. The authors presented two types of multitasking models to combine usage and definition modeling. First, the authors used the GRU-based context encoder model as a semantic generative network to generate word embedding. This approach encodes context sequences into continuous vectors and generates a fixed-size sentence embedding. After that, self-attention is applied to consider the target word sense. This model learns context-sensitive word embedding by fine-tuning Elmo models. Finally, the author formulated multitask sequence to sequence modeling for usage modeling to generate definition and example sentences.
