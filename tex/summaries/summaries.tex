\documentclass{article}[a4paper]
\usepackage[pdftex]{hyperref}
\usepackage[backend=bibtex,sorting=none]{biblatex}
\addbibresource{../paper.bib}

\newcommand{\bitem}[2]{
    \item[\cite{#1}]
        \citetitle{#1}, \citeauthor{#1}, \citeyear{#1}
        \newline
        {#2}
}%

\begin{document}

\begin{itemize}
    \bitem{noraset_definition_2016}%
    {%
        \textbf{Gist:}
        Describe the problem of definition modelling and generate defintions
        from an input word with RNN model.

        \textbf{Summary:}
        Definition modeling was intially described by
        \citeauthor{noraset_definition_2016}. Their research is based on a
        recurrent neural network language model \cite{mikolov_recurrent_2010}
        with a modified recurrent unit. They use the word to be defined placed
        at the beginning of the definition so the model will see the word only
        on the first step.

        \textbf{Evaluation:}
        Perplexity, BLEU

        \textbf{Dataset:}
        Custom - https://github.com/northanapon/dict-definition

        \textbf{Models:}
        RNN, Word2Vec, LSTM
    }%

    \bitem{chang_what_2019}%
    {%
        \textbf{Gist:}
        Instead of generating text, use a classification approach based on
        $k$-nn.

        \textbf{Summary:}
        \citeauthor{chang_what_2019} explore contextualized embedding for
        definition modeling. They reformulate the problem of definition modeling
        from text generation to text classification. Their results show
        state-of-the-art performance on the task of definition modeling.

        \textbf{Evaluation:}
        Precision, ROUGE-L, Cosine similarity

        \textbf{Dataset:}
        Oxford Dictionary

        \textbf{Models:}
        ELMo, BERT, fasttext
    }%

    \bitem{washio_bridging_2019}%
    {%
        \textbf{Gist:}
        Exploit lexical semantic relations between words to generate better
        definitions.

        \textbf{Summary:}
        \citeauthor{washio_bridging_2019} proposed a method for context-based
        definition modeling that considers the semantic relations between both
        the word to be defined and the words in the definition. They apply
        semantic information to both the definition encoder and decoder.

        \textbf{Evaluation:}
        Perplexity, BLEU

        \textbf{Dataset:}
        Dataset 1 \cite{noraset_definition_2016},
        Dataset 2 \cite{gadetsky_conditional_2018}

        \textbf{Models:}
        Encoder/decoder
    }%

    \bitem{barba_exemplification_2021}%
    {%
        \textbf{Gist:}
        Reverse definition modelling - generate examples (context sentences)
        from definition.

        \textbf{Summary:}
        \citeauthor{barba_exemplification_2021} introduce exemplification
        modeling, an adjacent problem to definition modeling that uses a
        definition embedding to generate possible example sentences. They use a
        sequence-to-sequence based approach and show near human-level annotation
        performance. Their problem is similar in that they use the definition as
        context to create example sentences.

        \textbf{Evaluation:}
        F1
    }%

    \bitem{gadetsky_conditional_2018}%
    {%
        \textbf{Gist:}

        \textbf{Summary:}

        \textbf{Evaluation:}
        Perplexity, BLEU

        \textbf{Dataset:}
        Oxford Dictionary

        \textbf{Models:}
        LSTM, Word2Vec, Skip-gram
    }%

    % \bitem{fourrier_methodological_2020}% {Summary}

    % \bitem{list_web_2017}% {Summary}

    % \bitem{demelo_etymological_2014}% {Summary}

    % \bitem{wettig_using_2012}% {Summary}

    % \bitem{ciobanu_etymological_2014}% {Summary}

    % \bitem{nouri_alignment_2016}% {Summary}

    % \bitem{dash_designing_2013}% {Summary}

    % \bitem{pyysalo_proto_2017}% {Summary}

    % \bitem{wu_2021_sequence}% {Summary}

    % \bitem{wu_computational_2020}% {Summary}

    % \bitem{zhou_density_2019}% {Summary}

    % \bitem{patra_bilingual_2019}% {Summary}

    % \bitem{beinborn_semantic_2020}% {Summary}

    % \bitem{cristea_towards_2021}% {Summary}

    % \bitem{bowers_deep_2016}% {Summary}

    % \bitem{mericli_annotating_2015}% {Summary}

    % \bitem{bhowmik_leveraging_2021}% {Summary}

    % \bitem{cheng_towards_2020}% {Summary}
\end{itemize}

\printbibliography

\end{document}
