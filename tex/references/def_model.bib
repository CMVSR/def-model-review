@article{noraset_definition_2016,
  title      = {Definition {Modeling}: {Learning} to define word embeddings in
                natural language},
  shorttitle = {Definition {Modeling}},
  url        = {http://arxiv.org/abs/1612.00394},
  abstract   = {Distributed representations
                of words have been shown to capture
                lexical semantics, as demonstrated by their effectiveness in
                word similarity and analogical relation tasks. But, these tasks
                only evaluate lexical semantics indirectly. In this paper, we
                study whether it is possible to utilize distributed
                representations to generate dictionary definitions of words, as
                a more direct and transparent representation of the embeddings'
                semantics. We introduce definition modeling, the task of
                generating a definition for a given word and its embedding. We
                present several definition model architectures based on
                recurrent neural networks, and experiment with the models over
                multiple data sets. Our results show that a model that controls
                dependencies between the word being defined and the definition
                words performs significantly better, and that a character-level
                convolution layer designed to leverage morphology can complement
                word-level embeddings. Finally, an error analysis suggests that
                the errors made by a definition model may provide insight into
                the shortcomings of word embeddings.},
  urldate    = {2021-10-19},
  journal    = {arXiv:1612.00394 [cs]},
  author     = {Noraset, Thanapon and Liang, Chen and Birnbaum, Larry and Downey, Doug},
  month      = dec,
  year       = {2016},
  note       = {arXiv: 1612.00394},
  keywords   = {Computer Science - Computation and Language}
}

@inproceedings{gadetsky_conditional_2018,
  address   = {Melbourne, Australia},
  title     = {Conditional {Generators} of
               {Words} {Definitions}},
  url       = {https://aclanthology.org/P18-2043},
  doi       = {10.18653/v1/P18-2043},
  abstract  = {We explore recently introduced
               definition modeling technique that provided the tool for
               evaluation of different distributed vector representations of
               words through modeling dictionary definitions of words. In this
               work, we study the problem of word ambiguities in definition
               modeling and propose a possible solution by employing latent
               variable modeling and soft attention mechanisms. Our quantitative
               and qualitative evaluation and analysis of the model shows that
               taking into account words' ambiguity and polysemy leads to
               performance improvement.},
  urldate   = {2022-01-12},
  booktitle = {Proceedings of the 56th
               {Annual} {Meeting} of the {Association} for {Computational}
               {Linguistics} ({Volume} 2: {Short} {Papers})},
  publisher = {Association for Computational
               Linguistics},
  author    = {Gadetsky, Artyom and Yakubovskiy, Ilya and Vetrov,
               Dmitry},
  month     = jul,
  year      = {2018},
  pages     = {266--271}
}

@inproceedings{chang_what_2019,
  address    = {Hong Kong, China},
  title      = {What {Does} {This} {Word}
                {Mean}? {Explaining} {Contextualized} {Embeddings} with
                {Natural} {Language} {Definition}},
  shorttitle = {What {Does} {This} {Word} {Mean}?},
  url        = {https://aclanthology.org/D19-1627},
  doi        = {10.18653/v1/D19-1627},
  abstract   = {Contextualized word embeddings have boosted many NLP tasks
                compared with traditional static word embeddings. However, the
                word with a specific sense may have different contextualized
                embeddings due to its various contexts. To further investigate
                what contextualized word embeddings capture, this paper analyzes
                whether they can indicate the corresponding sense definitions
                and proposes a general framework that is capable of explaining
                word meanings given contextualized word embeddings for better
                interpretation. The experiments show that both ELMo and BERT
                embeddings can be well interpreted via a readable textual form,
                and the findings may benefit the research community for a better
                understanding of what the embeddings capture.},
  urldate    = {2021-10-19},
  booktitle  = {Proceedings of the 2019 {Conference}
                on {Empirical} {Methods} in {Natural} {Language} {Processing}
                and the 9th {International} {Joint} {Conference} on {Natural}
                {Language} {Processing} ({EMNLP}-{IJCNLP})},
  publisher  = {Association for Computational Linguistics},
  author     = {Chang,
                Ting-Yun and Chen, Yun-Nung},
  month      = nov,
  year       = {2019},
  pages      = {6064--6070}
}

@inproceedings{washio_bridging_2019,
  address    = {Hong Kong, China},
  title      = {Bridging the {Defined} and the
                {Defining}: {Exploiting} {Implicit} {Lexical} {Semantic}
                {Relations} in {Definition} {Modeling}},
  shorttitle = {Bridging the {Defined} and the
                {Defining}},
  url        = {https://aclanthology.org/D19-1357},
  doi        = {10.18653/v1/D19-1357},
  abstract   = {Definition modeling includes acquiring
                word embeddings from dictionary definitions and generating
                definitions of words. While the meanings of defining words are
                important in dictionary definitions, it is crucial to capture
                the lexical semantic relations between defined words and
                defining words. However, thus far, the utilization of such
                relations has not been explored for definition modeling. In this
                paper, we propose definition modeling methods that use lexical
                semantic relations. To utilize implicit semantic relations in
                definitions, we use unsupervisedly obtained pattern-based
                word-pair embeddings that represent semantic relations of word
                pairs. Experimental results indicate that our methods improve
                the performance in learning embeddings from definitions, as well
                as definition generation.},
  urldate    = {2021-10-19},
  booktitle  = {Proceedings of the 2019 {Conference}
                on {Empirical} {Methods} in {Natural} {Language} {Processing}
                and the 9th {International} {Joint} {Conference} on {Natural}
                {Language} {Processing} ({EMNLP}-{IJCNLP})},
  publisher  = {Association for Computational Linguistics},
  author     = {Washio, Koki and Sekine, Satoshi and Kato, Tsuneaki},
  month      = nov,
  year       = {2019},
  pages      = {3521--3527}
}

@inproceedings{zheng_decompose_2021,
  address    = {Online},
  title      = {Decompose, {Fuse} and {Generate}: {A}
                {Formation}-{Informed} {Method} for {Chinese} {Definition}
                {Generation}},
  shorttitle = {Decompose, {Fuse} and {Generate}},
  url        = {https://aclanthology.org/2021.naacl-main.437},
  doi        = {10.18653/v1/2021.naacl-main.437},
  language   = {en},
  urldate    = {2022-01-18},
  booktitle  = {Proceedings of the 2021 {Conference} of the
                {North} {American} {Chapter} of the {Association} for
                {Computational} {Linguistics}: {Human} {Language}
                {Technologies}},
  publisher  = {Association
                for Computational Linguistics},
  author     = {Zheng, Hua and Dai, Damai and
                Li, Lei and Liu, Tianyu and Sui, Zhifang and Chang, Baobao and
                Liu, Yang},
  year       = {2021},
  pages      = {5524--5531}
}

@inproceedings{li_explicit_2020,
  address   = {Online},
  title     = {Explicit {Semantic} {Decomposition} for
               {Definition} {Generation}},
  url       = {https://www.aclweb.org/anthology/2020.acl-main.65},
  doi       = {10.18653/v1/2020.acl-main.65},
  language  = {en},
  urldate   = {2022-01-18},
  booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association}
               for {Computational} {Linguistics}},
  publisher = {Association for Computational
               Linguistics},
  author    = {Li, Jiahuan and Bao, Yu and Huang, Shujian and Dai,
               Xinyu and Chen, Jiajun},
  year      = {2020},
  pages     = {708--717}
}

@inproceedings{bevilacqua_generationary_2020,
  address   = {Online},
  title     = {Generationary or “{How} {We} {Went} beyond
               {Word} {Sense} {Inventories} and {Learned} to {Gloss}”},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-main.585},
  doi       = {10.18653/v1/2020.emnlp-main.585},
  language  = {en},
  urldate   = {2022-01-18},
  booktitle = {Proceedings of the 2020 {Conference} on {Empirical}
               {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
  publisher = {Association for
               Computational Linguistics},
  author    = {Bevilacqua, Michele and Maru, Marco
               and Navigli, Roberto},
  year      = {2020},
  pages     = {7207--7221}
}

@incollection{sojka_evaluating_2020,
  address   = {Cham},
  title     = {Evaluating a {Multi}-sense {Definition}
               {Generation} {Model} for {Multiple} {Languages}},
  volume    = {12284},
  isbn      = {9783030583224 9783030583231},
  url       = {https://link.springer.com/10.1007/978-3-030-58323-1_16},
  language  = {en},
  urldate   = {2022-01-18},
  booktitle = {Text, {Speech}, and {Dialogue}},
  publisher = {Springer International Publishing},
  author    = {Kabiri, Arman
               and Cook, Paul},
  editor    = {Sojka, Petr and Kopeček, Ivan and Pala, Karel
               and Horák, Aleš},
  year      = {2020},
  doi       = {10.1007/978-3-030-58323-1_16},
  pages     = {153--161}
}

@inproceedings{reid_vcdm_2020,
  address    = {Online},
  title      = {{VCDM}: {Leveraging} {Variational}
                {Bi}-encoding and {Deep} {Contextualized} {Word}
                {Representations} for {Improved} {Definition} {Modeling}},
  shorttitle = {{VCDM}},
  url        = {https://www.aclweb.org/anthology/2020.emnlp-main.513},
  doi        = {10.18653/v1/2020.emnlp-main.513},
  language   = {en},
  urldate    = {2022-01-18},
  booktitle  = {Proceedings of the 2020 {Conference} on
                {Empirical} {Methods} in {Natural} {Language} {Processing}
                ({EMNLP})},
  publisher  = {Association for Computational Linguistics},
  author     = {Reid,
                Machel and Marrese-Taylor, Edison and Matsuo, Yutaka},
  year       = {2020},
  pages      = {6331--6344}
}

@article{zhang_improving_2020,
  title    = {Improving interpretability of word embeddings by generating
              definition and usage},
  volume   = {160},
  issn     = {09574174},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0957417420304577},
  doi      = {10.1016/j.eswa.2020.113633},
  language = {en},
  urldate  = {2022-01-18},
  journal  = {Expert Systems with Applications},
  author   = {Zhang, Haitong and
              Du, Yongping and Sun, Jiaxin and Li, Qingxiao},
  month    = dec,
  year     = {2020},
  pages    = {113633}
}

@article{yang_incorporating_2020,
  title   = {Incorporating {Sememes} into {Chinese} {Definition} {Modeling}},
  volume  = {28},
  issn    = {2329-9290, 2329-9304},
  url     = {https://ieeexplore.ieee.org/document/9072279/},
  doi     = {10.1109/TASLP.2020.2987754},
  urldate = {2022-01-18},
  journal = {IEEE/ACM
             Transactions on Audio, Speech, and Language Processing},
  author  = {Yang,
             Liner and Kong, Cunliang and Chen, Yun and Liu, Yang and Fan, Qinan
             and Yang, Erhong},
  year    = {2020},
  pages   = {1669--1677}
}

@inproceedings{ishiwatari_learning_2019,
  address   = {Minneapolis, Minnesota},
  title     = {Learning to {Describe}
               {Unknown} {Phrases} with {Local} and {Global} {Contexts}},
  url       = {http://aclweb.org/anthology/N19-1350},
  doi       = {10.18653/v1/N19-1350},
  language  = {en},
  urldate   = {2022-01-18},
  booktitle = {Proceedings of the
               2019 {Conference} of the {North}},
  publisher = {Association for Computational
               Linguistics},
  author    = {Ishiwatari, Shonosuke and Hayashi, Hiroaki and
               Yoshinaga, Naoki and Neubig, Graham and Sato, Shoetsu and Toyoda,
               Masashi and Kitsuregawa, Masaru},
  year      = {2019},
  pages     = {3467--3476}
}

@inproceedings{ni_learning_2017,
  title     = {Learning to {Explain} {Non}-{Standard} {English} {Words} and
               {Phrases}},
  abstract  = {A data-driven approach for automatically explaining
               new, non-standard English expressions in a given sentence,
               building on a large dataset that includes 15 years of
               crowdsourced examples from UrbanDictionary.com, which can produce
               reasonable definitions of new non- standard English expressions
               given their context with certain confidence. We describe a
               data-driven approach for automatically explaining new,
               non-standard English expressions in a given sentence, building on
               a large dataset that includes 15 years of crowdsourced examples
               from UrbanDictionary.com. Unlike prior studies that focus on
               matching keywords from a slang dictionary, we investigate the
               possibility of learning a neural sequence-to-sequence model that
               generates explanations of unseen non-standard English expressions
               given context. We propose a dual encoder approach—a word-level
               encoder learns the representation of context, and a second
               character-level encoder to learn the hidden representation of the
               target non-standard expression. Our model can produce reasonable
               definitions of new non-standard English expressions given their
               context with certain confidence.},
  booktitle = {{IJCNLP}},
  author    = {Ni, Ke and Wang, William Yang},
  year      = {2017}
}

@article{huang_cdm_2021,
  title      = {{CDM}: {Combining} {Extraction} and {Generation} for
                {Definition} {Modeling}},
  shorttitle = {{CDM}},
  abstract   = {This paper
                proposes to combine extraction and generation for definition
                modeling: first extract self and correlative definitional
                information of target terms from the Web and then generate the
                final definitions by incorporating the extracted definitional
                Information. Definitions are essential for term understanding.
                Recently, there is an increasing interest in extracting and
                generating definitions of terms automatically. However, existing
                approaches for this task are either extractive or abstractive–
                definitions are either extracted from a corpus or generated by a
                language generation model. In this paper, we propose to combine
                extraction and generation for definition modeling: first extract
                selfand correlative definitional information of target terms
                from the Web and then generate the final definitions by
                incorporating the extracted definitional information.
                Experiments demonstrate our framework can generate high-quality
                definitions for technical terms and outperform state-of-the-art
                models for definition modeling significantly.1},
  journal    = {ArXiv},
  author     = {Huang, Jie and Shao, Hanyin and Chang,
                K.},
  year       = {2021}
}

@article{chang_xsense_2018,
  title      = {{xSense}: {Learning} {Sense}-{Separated} {Sparse}
                {Representations} and {Textual} {Definitions} for {Explainable}
                {Word} {Sense} {Networks}},
  shorttitle = {{xSense}},
  abstract   = {A large and high-quality
                context-definition dataset that consists of sense definitions
                together with multiple example sentences per polysemous word,
                which is a valuable resource for definition modeling and word
                sense disambiguation is introduced. Despite the success achieved
                on various natural language processing tasks, word embeddings
                are difficult to interpret due to the dense vector
                representations. This paper focuses on interpreting the
                embeddings for various aspects, including sense separation in
                the vector dimensions and definition generation. Specifically,
                given a context together with a target word, our algorithm first
                projects the target word embedding to a high-dimensional sparse
                vector and picks the specific dimensions that can best explain
                the semantic meaning of the target word by the encoded
                contextual information, where the sense of the target word can
                be indirectly inferred. Finally, our algorithm applies an RNN to
                generate the textual definition of the target word in the human
                readable form, which enables direct interpretation of the
                corresponding word embedding. This paper also introduces a large
                and high-quality context-definition dataset that consists of
                sense definitions together with multiple example sentences per
                polysemous word, which is a valuable resource for definition
                modeling and word sense disambiguation. The conducted
                experiments show the superior performance in BLEU score and the
                human evaluation test.},
  journal    = {ArXiv},
  author     = {Chang, Ting-Yun and Chi, Ta-Chung and
                Tsai, Shang-Chi and Chen, Yun-Nung (Vivian)},
  year       = {2018}
}

@article{kong_toward_2020,
  title    = {Toward {Cross}-{Lingual} {Definition} {Generation} for {Language}
              {Learners}},
  abstract = {The results show that the generated definitions are
              much simpler, which is more suitable for language learners, and
              the models have a strong cross-lingual transfer ability and can
              generate fluent English definitions for Chinese words. Generating
              dictionary definitions automatically can prove useful for language
              learners. However, it's still a challenging task of cross-lingual
              definition generation. In this work, we propose to generate
              definitions in English for words in various languages. To achieve
              this, we present a simple yet effective approach based on publicly
              available pretrained language models. In this approach, models can
              be directly applied to other languages after trained on the
              English dataset. We demonstrate the effectiveness of this approach
              on zero-shot definition generation. Experiments and manual
              analyses on newly constructed datasets show that our models have a
              strong cross-lingual transfer ability and can generate fluent
              English definitions for Chinese words. We further measure the
              lexical complexity of generated and reference definitions. The
              results show that the generated definitions are much simpler,
              which is more suitable for language learners.},
  journal  = {ArXiv},
  author   = {Kong, Cunliang and Yang, Liner and Zhang,
              Tianzuo and Fan, Qinan and Liu, Zhenghao and Chen, Yun-Nung and
              Yang, Erhong},
  year     = {2020}
}

@article{zhu_multi_2019,
  title    = {Multi-sense {Definition} {Modeling} using {Word} {Sense}
              {Decompositions}},
  abstract = {This paper introduces a new method that enables
              definition modeling for multiple senses of the same word, and
              shows how a Gumble-Softmax approach outperforms baselines at
              matching sense-specific embeddings to definitions during training.
              Word embeddings capture syntactic and semantic information about
              words. Definition modeling aims to make the semantic content in
              each embedding explicit, by outputting a natural language
              definition based on the embedding. However, existing definition
              models are limited in their ability to generate accurate
              definitions for different senses of the same word. In this paper,
              we introduce a new method that enables definition modeling for
              multiple senses. We show how a Gumble-Softmax approach outperforms
              baselines at matching sense-specific embeddings to definitions
              during training. In experiments, our multi-sense definition model
              improves recall over a state-of-the-art single-sense definition
              model by a factor of three, without harming precision.},
  journal  = {ArXiv},
  author   = {Zhu,
              Ruimin and Noraset, Thanapon and Liu, Alisa and Jiang, Wenxin and
              Downey, Doug},
  year     = {2019}
}

@article{mickus_mark_2019,
  title      = {Mark my {Word}: {A} {Sequence}-to-{Sequence} {Approach} to
                {Definition} {Modeling}},
  shorttitle = {Mark my {Word}},
  abstract   = {This
                proposal allows to train contextualization and definition
                generation in an end-to-end fashion, which is a conceptual
                improvement over earlier works and achieves state-of-the-art
                results both in contextual and non-contextual definition
                modeling. Defining words in a textual context is a useful task
                both for practical purposes and for gaining insight into
                distributed word representations. Building on the distributional
                hypothesis, we argue here that the most natural formalization of
                definition modeling is to treat it as a sequence-to-sequence
                task, rather than a word-to-sequence task: given an input
                sequence with a highlighted word, generate a contextually
                appropriate definition for it. We implement this approach in a
                Transformer-based sequence-to-sequence model. Our proposal
                allows to train contextualization and definition generation in
                an end-to-end fashion, which is a conceptual improvement over
                earlier works. We achieve state-of-the-art results both in
                contextual and non-contextual definition modeling.},
  journal    = {ArXiv},
  author     = {Mickus, Timothee and Paperno, Denis and
                Constant, M.},
  year       = {2019}
}
