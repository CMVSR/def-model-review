@inproceedings{bosc_auto_2018,
  address   = {Brussels, Belgium},
  title     = {Auto-{Encoding} {Dictionary}
               {Definitions} into {Consistent} {Word} {Embeddings}},
  url       = {http://aclweb.org/anthology/D18-1181},
  doi       = {10.18653/v1/D18-1181},
  language  = {en},
  urldate   = {2022-01-18},
  booktitle = {Proceedings of the
               2018 {Conference} on {Empirical} {Methods} in {Natural}
               {Language} {Processing}},
  publisher = {Association for Computational Linguistics},
  author    = {Bosc,
               Tom and Vincent, Pascal},
  year      = {2018},
  pages     = {1522--1532}
}

@inproceedings{kaneko_dictionary_2021,
  address   = {Online},
  title     = {Dictionary-based {Debiasing} of
               {Pre}-trained {Word} {Embeddings}},
  url       = {https://aclanthology.org/2021.eacl-main.16},
  doi       = {10.18653/v1/2021.eacl-main.16},
  language  = {en},
  urldate   = {2022-01-18},
  booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter}
               of the {Association} for {Computational} {Linguistics}: {Main}
               {Volume}},
  publisher = {Association for Computational Linguistics},
  author    = {Kaneko,
               Masahiro and Bollegala, Danushka},
  year      = {2021},
  pages     = {212--223}
}

@inproceedings{zheng_decompose_2021,
  address    = {Online},
  title      = {Decompose, {Fuse} and {Generate}: {A}
                {Formation}-{Informed} {Method} for {Chinese} {Definition}
                {Generation}},
  shorttitle = {Decompose, {Fuse} and {Generate}},
  url        = {https://aclanthology.org/2021.naacl-main.437},
  doi        = {10.18653/v1/2021.naacl-main.437},
  language   = {en},
  urldate    = {2022-01-18},
  booktitle  = {Proceedings of the 2021 {Conference} of the
                {North} {American} {Chapter} of the {Association} for
                {Computational} {Linguistics}: {Human} {Language}
                {Technologies}},
  publisher  = {Association
                for Computational Linguistics},
  author     = {Zheng, Hua and Dai, Damai and
                Li, Lei and Liu, Tianyu and Sui, Zhifang and Chang, Baobao and
                Liu, Yang},
  year       = {2021},
  pages      = {5524--5531}
}

@inproceedings{li_explicit_2020,
  address   = {Online},
  title     = {Explicit {Semantic} {Decomposition} for
               {Definition} {Generation}},
  url       = {https://www.aclweb.org/anthology/2020.acl-main.65},
  doi       = {10.18653/v1/2020.acl-main.65},
  language  = {en},
  urldate   = {2022-01-18},
  booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association}
               for {Computational} {Linguistics}},
  publisher = {Association for Computational
               Linguistics},
  author    = {Li, Jiahuan and Bao, Yu and Huang, Shujian and Dai,
               Xinyu and Chen, Jiajun},
  year      = {2020},
  pages     = {708--717}
}

@inproceedings{bevilacqua_generationary_2020,
  address   = {Online},
  title     = {Generationary or “{How} {We} {Went} beyond
               {Word} {Sense} {Inventories} and {Learned} to {Gloss}”},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-main.585},
  doi       = {10.18653/v1/2020.emnlp-main.585},
  language  = {en},
  urldate   = {2022-01-18},
  booktitle = {Proceedings of the 2020 {Conference} on {Empirical}
               {Methods} in
               {Natural} {Language} {Processing} ({EMNLP})},
  publisher = {Association for
               Computational Linguistics},
  author    = {Bevilacqua, Michele and Maru, Marco
               and Navigli, Roberto},
  year      = {2020},
  pages     = {7207--7221}
}

@incollection{sojka_evaluating_2020,
  address   = {Cham},
  title     = {Evaluating a {Multi}-sense {Definition}
               {Generation} {Model} for {Multiple} {Languages}},
  volume    = {12284},
  isbn      = {9783030583224 9783030583231},
  url       = {https://link.springer.com/10.1007/978-3-030-58323-1_16},
  language  = {en},
  urldate   = {2022-01-18},
  booktitle = {Text, {Speech}, and {Dialogue}},
  publisher = {Springer International Publishing},
  author    = {Kabiri, Arman
               and Cook, Paul},
  editor    = {Sojka, Petr and Kopeček, Ivan and Pala, Karel
               and Horák, Aleš},
  year      = {2020},
  doi       = {10.1007/978-3-030-58323-1_16},
  pages     = {153--161}
}

@inproceedings{reid_vcdm_2020,
  address    = {Online},
  title      = {{VCDM}: {Leveraging} {Variational}
                {Bi}-encoding and {Deep} {Contextualized} {Word}
                {Representations} for {Improved} {Definition} {Modeling}},
  shorttitle = {{VCDM}},
  url        = {https://www.aclweb.org/anthology/2020.emnlp-main.513},
  doi        = {10.18653/v1/2020.emnlp-main.513},
  language   = {en},
  urldate    = {2022-01-18},
  booktitle  = {Proceedings of the 2020 {Conference} on
                {Empirical} {Methods} in {Natural} {Language} {Processing}
                ({EMNLP})},
  publisher  = {Association for Computational Linguistics},
  author     = {Reid,
                Machel and Marrese-Taylor, Edison and Matsuo, Yutaka},
  year       = {2020},
  pages      = {6331--6344}
}

@article{zhang_improving_2020,
  title    = {Improving interpretability of word embeddings by generating
              definition and usage},
  volume   = {160},
  issn     = {09574174},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0957417420304577},
  doi      = {10.1016/j.eswa.2020.113633},
  language = {en},
  urldate  = {2022-01-18},
  journal  = {Expert Systems with Applications},
  author   = {Zhang, Haitong and
              Du, Yongping and Sun, Jiaxin and Li, Qingxiao},
  month    = dec,
  year     = {2020},
  pages    = {113633}
}

@article{yang_incorporating_2020,
  title   = {Incorporating {Sememes} into {Chinese} {Definition} {Modeling}},
  volume  = {28},
  issn    = {2329-9290, 2329-9304},
  url     = {https://ieeexplore.ieee.org/document/9072279/},
  doi     = {10.1109/TASLP.2020.2987754},
  urldate = {2022-01-18},
  journal = {IEEE/ACM
             Transactions on Audio, Speech, and Language Processing},
  author  = {Yang,
             Liner and Kong, Cunliang and Chen, Yun and Liu, Yang and Fan, Qinan
             and Yang, Erhong},
  year    = {2020},
  pages   = {1669--1677}
}

@inproceedings{ishiwatari_learning_2019,
  address   = {Minneapolis, Minnesota},
  title     = {Learning to {Describe}
               {Unknown} {Phrases} with {Local} and {Global} {Contexts}},
  url       = {http://aclweb.org/anthology/N19-1350},
  doi       = {10.18653/v1/N19-1350},
  language  = {en},
  urldate   = {2022-01-18},
  booktitle = {Proceedings of the
               2019 {Conference} of the {North}},
  publisher = {Association for Computational
               Linguistics},
  author    = {Ishiwatari, Shonosuke and Hayashi, Hiroaki and
               Yoshinaga, Naoki and Neubig, Graham and Sato, Shoetsu and Toyoda,
               Masashi and Kitsuregawa, Masaru},
  year      = {2019},
  pages     = {3467--3476}
}

@inproceedings{ni_learning_2017,
  title     = {Learning to {Explain} {Non}-{Standard} {English} {Words} and
               {Phrases}},
  abstract  = {A data-driven approach for automatically explaining
               new, non-standard English expressions in a given sentence,
               building on a large dataset that includes 15 years of
               crowdsourced examples from UrbanDictionary.com, which can produce
               reasonable definitions of new non- standard English expressions
               given their context with certain confidence. We describe a
               data-driven approach for automatically explaining new,
               non-standard English expressions in a given sentence, building on
               a large dataset that includes 15 years of crowdsourced examples
               from UrbanDictionary.com. Unlike prior studies that focus on
               matching keywords from a slang dictionary, we investigate the
               possibility of learning a neural sequence-to-sequence model that
               generates explanations of unseen non-standard English expressions
               given context. We propose a dual encoder approach—a word-level
               encoder learns the representation of context, and a second
               character-level encoder to learn the hidden representation of the
               target non-standard expression. Our model can produce reasonable
               definitions of new non-standard English expressions given their
               context with certain confidence.},
  booktitle = {{IJCNLP}},
  author    = {Ni, Ke and Wang, William Yang},
  year      = {2017}
}

@article{ishiwatari_learning_2018,
  title    = {Learning to {Describe} {Phrases} with {Local} and {Global}
              {Contexts}},
  abstract = {A neural description model that consists of two
              context encoders and a description decoder that appropriately
              takes important clues from both local and global contexts is
              proposed. When reading a text, it is common to become stuck on
              unfamiliar words and phrases, such as polysemous words with novel
              senses, rarely used idioms, internet slang, or emerging entities.
              If we humans cannot figure out the meaning of those expressions
              from the immediate local context, we consult dictionaries for
              definitions or search documents or the web to find other global
              context to help in interpretation. Can machines help us do this
              work? Which type of context is more important for machines to
              solve the problem? To answer these questions, we undertake a task
              of describing a given phrase in natural language based on its
              local and global contexts. To solve this task, we propose a neural
              description model that consists of two context encoders and a
              description decoder. In contrast to the existing methods for
              non-standard English explanation [Ni+ 2017] and definition
              generation [Noraset+ 2017; Gadetsky+ 2018], our model
              appropriately takes important clues from both local and global
              contexts. Experimental results on three existing datasets
              (including WordNet, Oxford and Urban Dictionaries) and a dataset
              newly created from Wikipedia demonstrate the effectiveness of our
              method over previous work.},
  journal  = {ArXiv},
  author   = {Ishiwatari, Shonosuke and Hayashi, Hiroaki and
              Yoshinaga, Naoki and Neubig,
              Graham and Toyoda, M. and Kitsuregawa, M.},
  year     = {2018}
}

@article{huang_cdm_2021,
  title      = {{CDM}: {Combining} {Extraction} and {Generation} for
                {Definition} {Modeling}},
  shorttitle = {{CDM}},
  abstract   = {This paper
                proposes to combine extraction and generation for definition
                modeling: first extract self and correlative definitional
                information of target terms from the Web and then generate the
                final definitions by incorporating the extracted definitional
                Information. Definitions are essential for term understanding.
                Recently, there is an increasing interest in extracting and
                generating definitions of terms automatically. However, existing
                approaches for this task are either extractive or abstractive–
                definitions are either extracted from a corpus or generated by a
                language generation model. In this paper, we propose to combine
                extraction and generation for definition modeling: first extract
                selfand correlative definitional information of target terms
                from the Web and then generate the final definitions by
                incorporating the extracted definitional information.
                Experiments demonstrate our framework can generate high-quality
                definitions for technical terms and outperform state-of-the-art
                models for definition modeling significantly.1},
  journal    = {ArXiv},
  author     = {Huang, Jie and Shao, Hanyin and Chang,
                K.},
  year       = {2021}
}

@article{chang_xsense_2018,
  title      = {{xSense}: {Learning} {Sense}-{Separated} {Sparse}
                {Representations} and {Textual} {Definitions} for {Explainable}
                {Word} {Sense} {Networks}},
  shorttitle = {{xSense}},
  abstract   = {A large and high-quality
                context-definition dataset that consists of sense definitions
                together with multiple example sentences per polysemous word,
                which is a valuable resource for definition modeling and word
                sense disambiguation is introduced. Despite the success achieved
                on various natural language processing tasks, word embeddings
                are difficult to interpret due to the dense vector
                representations. This paper focuses on interpreting the
                embeddings for various aspects, including sense separation in
                the vector dimensions and definition generation. Specifically,
                given a context together with a target word, our algorithm first
                projects the target word embedding to a high-dimensional sparse
                vector and picks the specific dimensions that can best explain
                the semantic meaning of the target word by the encoded
                contextual information, where the sense of the target word can
                be indirectly inferred. Finally, our algorithm applies an RNN to
                generate the textual definition of the target word in the human
                readable form, which enables direct interpretation of the
                corresponding word embedding. This paper also introduces a large
                and high-quality context-definition dataset that consists of
                sense definitions together with multiple example sentences per
                polysemous word, which is a valuable resource for definition
                modeling and word sense disambiguation. The conducted
                experiments show the superior performance in BLEU score and the
                human evaluation test.},
  journal    = {ArXiv},
  author     = {Chang, Ting-Yun and Chi, Ta-Chung and
                Tsai, Shang-Chi
                and Chen, Yun-Nung (Vivian)},
  year       = {2018}
}

@article{kong_toward_2020,
  title    = {Toward {Cross}-{Lingual} {Definition} {Generation} for {Language}
              {Learners}},
  abstract = {The results show that the generated definitions are
              much simpler, which is more suitable for language learners, and
              the models have a strong cross-lingual transfer ability and can
              generate fluent English definitions for Chinese words. Generating
              dictionary definitions automatically can prove useful for language
              learners. However, it's still a challenging task of cross-lingual
              definition generation. In this work, we propose to generate
              definitions in English for words in various languages. To achieve
              this, we present a simple yet effective approach based on publicly
              available pretrained language models. In this approach, models can
              be directly applied to other languages after trained on the
              English dataset. We demonstrate the effectiveness of this approach
              on zero-shot definition generation. Experiments and manual
              analyses on newly constructed datasets show that our models have a
              strong cross-lingual transfer ability and can generate fluent
              English definitions for Chinese words. We further measure the
              lexical complexity of generated and reference definitions. The
              results show that the generated definitions are much simpler,
              which is more suitable for language learners.},
  journal  = {ArXiv},
  author   = {Kong, Cunliang and Yang, Liner and Zhang,
              Tianzuo and Fan, Qinan and Liu, Zhenghao and Chen, Yun-Nung and
              Yang, Erhong},
  year     = {2020}
}

@article{zhu_multi_2019,
  title    = {Multi-sense {Definition} {Modeling} using {Word} {Sense}
              {Decompositions}},
  abstract = {This paper introduces a new method that enables
              definition modeling for multiple senses of the same word, and
              shows how a Gumble-Softmax approach outperforms baselines at
              matching sense-specific embeddings to definitions during training.
              Word embeddings capture syntactic and semantic information about
              words. Definition modeling aims to make the semantic content in
              each embedding explicit, by outputting a natural language
              definition based on the embedding. However, existing definition
              models are limited in their ability to generate accurate
              definitions for different senses of the same word. In this paper,
              we introduce a new method that enables definition modeling for
              multiple senses. We show how a Gumble-Softmax approach outperforms
              baselines at matching sense-specific embeddings to definitions
              during training. In experiments, our multi-sense definition model
              improves recall over a state-of-the-art single-sense definition
              model by a factor of three, without harming precision.},
  journal  = {ArXiv},
  author   = {Zhu,
              Ruimin and Noraset, Thanapon and Liu, Alisa and Jiang, Wenxin and
              Downey, Doug},
  year     = {2019}
}

@article{mickus_mark_2019,
  title      = {Mark my {Word}: {A} {Sequence}-to-{Sequence} {Approach} to
                {Definition} {Modeling}},
  shorttitle = {Mark my {Word}},
  abstract   = {This
                proposal allows to train contextualization and definition
                generation in an end-to-end fashion, which is a conceptual
                improvement over earlier works and achieves state-of-the-art
                results both in contextual and non-contextual definition
                modeling. Defining words in a textual context is a useful task
                both for practical purposes and for gaining insight into
                distributed word representations. Building on the distributional
                hypothesis, we argue here that the most natural formalization of
                definition modeling is to treat it as a sequence-to-sequence
                task, rather than a word-to-sequence task: given an input
                sequence with a highlighted word, generate a contextually
                appropriate definition for it. We implement this approach in a
                Transformer-based sequence-to-sequence model. Our proposal
                allows to train contextualization and definition generation in
                an end-to-end fashion, which is a conceptual improvement over
                earlier works. We achieve state-of-the-art results both in
                contextual and non-contextual definition modeling.},
  journal    = {ArXiv},
  author     = {Mickus, Timothee and Paperno, Denis and
                Constant, M.},
  year       = {2019}
}
