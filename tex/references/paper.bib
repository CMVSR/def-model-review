% recurrent neural network language model


@inproceedings{nastase-strapparava-2013-bridging,
    title = "Bridging Languages through Etymology: The case of cross language text categorization",
    author = "Nastase, Vivi  and
      Strapparava, Carlo",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-1064",
    pages = "651--659",
}
@inproceedings{Sagot2017ExtractingAE,
  title={Extracting an Etymological Database from Wiktionary},
  author={Beno{\^i}t Sagot},
  year={2017}
}
@inproceedings{beinborn-etal-2013-cognate,
    title = "Cognate Production using Character-based Machine Translation",
    author = "Beinborn, Lisa  and
      Zesch, Torsten  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the Sixth International Joint Conference on Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Nagoya, Japan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I13-1112",
    pages = "883--891",
}

@inproceedings{mikolov_recurrent_2010,
  title     = {Recurrent neural network based language model},
  url       = {http://www.isca-speech.org/archive/interspeech\_2010/i10\_1045.html},
  urldate   = {2021-10-19},
  booktitle = {{INTERSPEECH} 2010, 11th {Annual} {Conference} of the {International} {Speech} {Communication} {Association}, {Makuhari}, {Chiba}, {Japan}, {September} 26-30, 2010},
  publisher = {ISCA},
  author    = {Mikolov, Tomás and Karafiát, Martin and Burget, Lukás and Cernocký, Jan and Khudanpur, Sanjeev},
  editor    = {Kobayashi, Takao and Hirose, Keikichi and Nakamura, Satoshi},
  year      = {2010},
  pages     = {1045--1048}
}


@article{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2022-03-25},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv: 1301.3781},
	keywords = {Computer Science - Computation and Language},
}


% datasets
@inproceedings{miller_semantic_1993,
  title     = {A Semantic Concordance},
  author    = {Miller, George A.  and
               Leacock, Claudia  and
               Tengi, Randee  and
               Bunker, Ross T.},
  booktitle = {{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held
               at Plainsboro, New Jersey, March 21-24, 1993},
  year      = {1993},
  url       = {https://aclanthology.org/H93-1061}
}

% word embeddings
@inproceedings{mikolov_distributed_2013,
  title     = {Distributed Representations of Words and Phrases and their
               Compositionality},
  author    = {Tomas Mikolov and Ilya Sutskever and Kai Chen
               and Gregory S. Corrado and Jeffrey Dean},
  booktitle = {NIPS},
  year      = {2013}
}

% definition-informed embeddings
@article{hill_learning_2016,
  title    = {Learning to {Understand} {Phrases} by {Embedding} the
              {Dictionary}},
  volume   = {4},
  issn     = {2307-387X},
  url      = {https://direct.mit.edu/tacl/article/43354},
  doi      = {10.1162/tacl_a_00080},
  abstract = {Distributional models that learn rich
              semantic word representations are a success story of recent NLP research.
              However, developing models that learn useful representations of phrases and
              sentences has proved far harder. We propose using the definitions found in
              everyday dictionaries as a means of bridging this gap between lexical and
              phrasal semantics. Neural language embedding models can be effectively trained
              to map dictionary definitions (phrases) to (lexical) representations of the
              words defined by those definitions. We present two applications of these
              architectures: reverse dictionaries that return the name of a concept given a
              definition or description and general-knowledge crossword question answerers.
              On both tasks, neural language embedding models trained on definitions from a
              handful of freely-available lexical resources perform as well or better than
              existing commercial systems that rely on significant task-specific
              engineering. The results highlight the effectiveness of both neural embedding
              architectures and definition-based training for developing models that
              understand phrases and sentences.},
  language = {en},
  urldate  = {2022-02-01},
  journal  = {Transactions of the Association for Computational Linguistics},
  author   = {Hill, Felix and Cho, Kyunghyun and Korhonen, Anna and Bengio,
              Yoshua},
  month    = dec,
  year     = {2016},
  pages    = {17--30}
}

@inproceedings{bosc_auto_2018,
  address   = {Brussels, Belgium},
  title     = {Auto-{Encoding} {Dictionary}
               {Definitions} into {Consistent} {Word} {Embeddings}},
  url       = {http://aclweb.org/anthology/D18-1181},
  doi       = {10.18653/v1/D18-1181},
  language  = {en},
  urldate   = {2022-01-18},
  booktitle = {Proceedings of the
               2018 {Conference} on {Empirical} {Methods} in {Natural}
               {Language} {Processing}},
  publisher = {Association for Computational Linguistics},
  author    = {Bosc,
               Tom and Vincent, Pascal},
  year      = {2018},
  pages     = {1522--1532}
}

% bilingual and multilingual word embeddings
@inproceedings{zhou_density_2019,
  address   = {Minneapolis, Minnesota},
  title     = {Density {Matching} for
               {Bilingual} {Word} {Embedding}},
  url       = {http://aclweb.org/anthology/N19-1161},
  doi       = {10.18653/v1/N19-1161},
  language  = {en},
  urldate   = {2021-12-20},
  booktitle = {Proceedings of the
               2019 {Conference} of the {North}},
  publisher = {Association for Computational
               Linguistics},
  author    = {Zhou, Chunting and Ma, Xuezhe and Wang, Di and
               Neubig, Graham},
  year      = {2019},
  pages     = {1588--1598}
}

@inproceedings{patra_bilingual_2019,
  address   = {Florence, Italy},
  title     = {Bilingual {Lexicon} {Induction}
               with {Semi}-supervision in {Non}-{Isometric} {Embedding} {Spaces}},
  url       = {https://www.aclweb.org/anthology/P19-1018},
  doi       = {10.18653/v1/P19-1018},
  language  = {en},
  urldate   = {2021-12-20},
  booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for
               {Computational} {Linguistics}},
  publisher = {Association for Computational
               Linguistics},
  author    = {Patra, Barun and Moniz, Joel Ruben Antony and Garg,
               Sarthak and Gormley, Matthew R. and Neubig, Graham},
  year      = {2019},
  pages     = {184--193}
}

@article{beinborn_semantic_2020,
  title    = {Semantic {Drift} in {Multilingual} {Representations}},
  volume   = {46},
  issn     = {0891-2017, 1530-9312},
  url      = {https://direct.mit.edu/coli/article/46/3/571-603/93376},
  doi      = {10.1162/coli_a_00382},
  abstract = {Multilingual representations have mostly
              been evaluated based on their performance on specific tasks. In this article,
              we look beyond engineering goals and analyze the relations between languages
              in computational representations. We introduce a methodology for comparing
              languages based on their organization of semantic concepts. We propose to
              conduct an adapted version of representational similarity analysis of a
              selected set of concepts in computational multilingual representations. Using
              this analysis method, we can reconstruct a phylogenetic tree that closely
              resembles those assumed by linguistic experts. These results indicate that
              multilingual distributional representations that are only trained on
              monolingual text and bilingual dictionaries preserve relations between
              languages without the need for any etymological information. In addition, we
              propose a measure to identify semantic drift between language families. We
              perform experiments on word-based and sentence-based multilingual models and
              provide both quantitative results and qualitative examples. Analyses of
              semantic drift in multilingual representations can serve two purposes: They
              can indicate unwanted characteristics of the computational models and they
              provide a quantitative means to study linguistic phenomena across languages.},
  language = {en},
  number   = {3},
  urldate  = {2021-12-20},
  journal  = {Computational Linguistics},
  author   = {Beinborn, Lisa and Choenni,
              Rochelle},
  month    = nov,
  year     = {2020},
  pages    = {571--603}
}

@article{bhowmik_leveraging_2021,
  title      = {Leveraging {Vector} {Space} {Similarity} for {Learning}
                {Cross}-{Lingual} {Word} {Embeddings}: {A} {Systematic} {Review}},
  volume     = {1},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  shorttitle = {Leveraging {Vector} {Space} {Similarity} for {Learning} {Cross}-{Lingual}
                {Word} {Embeddings}},
  url        = {https://www.mdpi.com/2673-6470/1/3/11},
  doi        = {10.3390/digital1030011},
  abstract   = {This article presents a
                systematic literature review on quantifying the proximity between
                independently trained monolingual word embedding spaces. A search was carried
                out in the broader context of inducing bilingual lexicons from cross-lingual
                word embeddings, especially for low-resource languages. The returned articles
                were then classified. Cross-lingual word embeddings have drawn the attention
                of researchers in the field of natural language processing (NLP). Although
                existing methods have yielded satisfactory results for resource-rich languages
                and languages related to them, some researchers have pointed out that the same
                is not true for low-resource and distant languages. In this paper, we report
                the research on methods proposed to provide better representation for
                low-resource and distant languages in the cross-lingual word embedding
                space.},
  language   = {en},
  number     = {3},
  urldate    = {2021-12-20},
  journal    = {Digital},
  author     = {Bhowmik, Kowshik and Ralescu, Anca},
  month      = sep,
  year       = {2021},
  keywords   = {cross-lingual word
                embedding, orthogonal mapping, isomorphic assumption, distant languages,
                low-resource languages},
  pages      = {145--161}
}

% definition applications
@inproceedings{barba_exemplification_2021,
  address    = {Montreal, Canada},
  title      = {Exemplification {Modeling}:
                {Can} {You} {Give} {Me} an {Example}, {Please}?},
  isbn       = {9780999241196},
  shorttitle = {Exemplification {Modeling}},
  url        = {https://www.ijcai.org/proceedings/2021/520},
  doi        = {10.24963/ijcai.2021/520},
  abstract   = {Recently, generative approaches have
                been used effectively to
                provide definitions of words in their context. However, the
                opposite, i.e., generating a usage example given one or more
                words along with their definitions, has not yet been
                investigated. In this work, we introduce the novel task of
                Exemplification Modeling (ExMod), along with a
                sequence-to-sequence architecture and a training procedure for
                it. Starting from a set of (word, definition) pairs, our
                approach is capable of automatically generating high-quality
                sentences which express the requested semantics. As a result, we
                can drive the creation of sense-tagged data which cover the full
                range of meanings in any inventory of interest, and their
                interactions within sentences. Human annotators agree that the
                sentences generated are as fluent and semantically-coherent with
                the input definitions as the sentences in manually-annotated
                corpora. Indeed, when employed as training data for Word Sense
                Disambiguation, our examples enable the current state of the art
                to be outperformed, and higher results to be achieved than when
                using gold-standard datasets only. We release the pretrained
                model, the dataset and the software at
                https://github.com/SapienzaNLP/exmod.},
  language   = {en},
  urldate    = {2021-10-19},
  booktitle  = {Proceedings of the
                {Thirtieth} {International} {Joint} {Conference} on {Artificial}
                {Intelligence}},
  publisher  = {International Joint Conferences on Artificial
                Intelligence Organization},
  author     = {Barba, Edoardo and Procopio, Luigi
                and Lacerra, Caterina and Pasini, Tommaso and Navigli, Roberto},
  month      = aug,
  year       = {2021},
  pages      = {3779--3785}
}

@inproceedings{lewis2019bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}


@inproceedings{devlin2018bert,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-03-25},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}



@inproceedings{lin_2004_rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@inproceedings{papineni_2002_bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{banerjee_2005_meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909",
    pages = "65--72",
}


@article{zhang_bertscore_2020,
	title = {{BERTScore}: {Evaluating} {Text} {Generation} with {BERT}},
	shorttitle = {{BERTScore}},
	url = {http://arxiv.org/abs/1904.09675},
	abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.},
	urldate = {2022-03-25},
	journal = {arXiv:1904.09675 [cs]},
	author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	month = feb,
	year = {2020},
	note = {arXiv: 1904.09675},
	keywords = {Computer Science - Computation and Language},
}
